# Open-ended-metrics-for-LLMs-evaluation
Classification of open-ended metrics to evaluate NLP open-ended tasks with LLMs


This report presents a unified taxonomy and practical guidance for evaluating open-ended NLP tasks—such as summarization, machine translation, dialogue generation, question answering, paraphrasing and text generation—by classifying over 50 automatic and human metrics along axes of reference dependency, evaluation type, automation level, cost, and reliability etc. Drawing on a curated Excel dataset and insights from more than 27 recent surveys, papers and benchmarks (e.g., GEM, SummEval), we organize metrics into categories (lexical overlap, embedding similarity, learned model–based, QA-based, diversity, and safety measures) and analyze their strengths and limitations. For each task, we recommend specific metric combinations—e.g., ROUGE + BERTScore + QAGS for summarization, USL-H + Distinct-n for dialogue, MAUVE + Distinct-n for story generation—and provide a decision flowchart to match evaluation needs with resources and quality priorities. Our work equips practitioners and researchers with a clear framework to select and supplement metrics for robust, task-appropriate LLM evaluation.

### The repository will be update soon.
